{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image classification with a small CNN (cats vs dogs)\n",
        "\n",
        "Adapted from Chapter 8 of Fran\u00e7ois Chollet's *Deep Learning with Python*.\n",
        "\n",
        "This version **does not rely on Kaggle**. Instead, it uses a public Google-hosted\n",
        "mirror of the filtered cats vs dogs dataset:\n",
        "https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
        "\n",
        "It also creates a proper **train / validation / test** split by moving a subset of\n",
        "images into a new `test/` directory:\n",
        "\n",
        "- 200 cats + 200 dogs from `train/`\n",
        "- 100 cats + 100 dogs from `validation/`\n",
        "\n",
        "You can run this notebook directly in Colab. A GPU is recommended but not required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-imports"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download and extract the cats vs dogs dataset (Google mirror)\n",
        "\n",
        "We download the zip file to Keras' cache directory and then **explicitly extract** it.\n",
        "After extraction, we robustly locate the `cats_and_dogs_filtered` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download-and-extract"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "URL = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
        "\n",
        "# Download the zip file (without auto-extraction)\n",
        "zip_path = tf.keras.utils.get_file(\n",
        "    \"cats_and_dogs_filtered.zip\",\n",
        "    origin=URL,\n",
        "    extract=False,\n",
        ")\n",
        "zip_path = Path(zip_path)\n",
        "download_root = zip_path.parent\n",
        "print(\"Zip path:\", zip_path)\n",
        "print(\"Download root:\", download_root)\n",
        "\n",
        "# Explicitly extract the zip under download_root\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "    zf.extractall(download_root)\n",
        "\n",
        "# Locate the extracted 'cats_and_dogs_filtered' directory robustly\n",
        "candidates = [p for p in download_root.rglob(\"cats_and_dogs_filtered\") if p.is_dir()]\n",
        "if not candidates:\n",
        "    raise RuntimeError(f\"Could not find 'cats_and_dogs_filtered' under {download_root}\")\n",
        "\n",
        "base_dir = candidates[0]\n",
        "print(\"Using base_dir:\", base_dir)\n",
        "\n",
        "train_dir = base_dir / \"train\"\n",
        "validation_dir = base_dir / \"validation\"\n",
        "\n",
        "train_cats_dir = train_dir / \"cats\"\n",
        "train_dogs_dir = train_dir / \"dogs\"\n",
        "validation_cats_dir = validation_dir / \"cats\"\n",
        "validation_dogs_dir = validation_dir / \"dogs\"\n",
        "\n",
        "# Count images before any splitting\n",
        "num_train_cats = len(list(train_cats_dir.glob(\"*.jpg\")))\n",
        "num_train_dogs = len(list(train_dogs_dir.glob(\"*.jpg\")))\n",
        "num_val_cats = len(list(validation_cats_dir.glob(\"*.jpg\")))\n",
        "num_val_dogs = len(list(validation_dogs_dir.glob(\"*.jpg\")))\n",
        "\n",
        "print(\"BEFORE SPLIT:\")\n",
        "print(\"Training cats:\", num_train_cats)\n",
        "print(\"Training dogs:\", num_train_dogs)\n",
        "print(\"Validation cats:\", num_val_cats)\n",
        "print(\"Validation dogs:\", num_val_dogs)\n",
        "\n",
        "# Optional quick directory peek if counts are 0\n",
        "if (\n",
        "    num_train_cats == 0\n",
        "    and num_train_dogs == 0\n",
        "    and num_val_cats == 0\n",
        "    and num_val_dogs == 0\n",
        "):\n",
        "    print(\"\\nFirst 40 paths under base_dir for debugging:\")\n",
        "    for p in list(base_dir.rglob(\"*\"))[:40]:\n",
        "        print(\"  \", p)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a proper test split\n",
        "\n",
        "We create a new `test/` directory and **move** images into it so the test data is\n",
        "never seen during training:\n",
        "\n",
        "- 200 cats + 200 dogs from `train/`\n",
        "- 100 cats + 100 dogs from `validation/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create-test-split"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Reproducibility\n",
        "random.seed(1337)\n",
        "\n",
        "# Create test directories\n",
        "test_dir = base_dir / \"test\"\n",
        "test_cats_dir = test_dir / \"cats\"\n",
        "test_dogs_dir = test_dir / \"dogs\"\n",
        "\n",
        "for d in [test_dir, test_cats_dir, test_dogs_dir]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def move_subset(src_dir, dst_dir, n):\n",
        "    files = list(src_dir.glob(\"*.jpg\"))\n",
        "    if len(files) < n:\n",
        "        print(f\"Warning: requested {n} files from {src_dir}, but only {len(files)} available. Using all.\")\n",
        "        n = len(files)\n",
        "    selected = random.sample(files, n)\n",
        "    for p in selected:\n",
        "        shutil.move(str(p), dst_dir / p.name)\n",
        "\n",
        "# 200 from train (cats & dogs)\n",
        "move_subset(train_cats_dir, test_cats_dir, 200)\n",
        "move_subset(train_dogs_dir, test_dogs_dir, 200)\n",
        "\n",
        "# 100 from validation (cats & dogs)\n",
        "move_subset(validation_cats_dir, test_cats_dir, 100)\n",
        "move_subset(validation_dogs_dir, test_dogs_dir, 100)\n",
        "\n",
        "# Recount after the move\n",
        "num_train_cats = len(list(train_cats_dir.glob(\"*.jpg\")))\n",
        "num_train_dogs = len(list(train_dogs_dir.glob(\"*.jpg\")))\n",
        "num_val_cats = len(list(validation_cats_dir.glob(\"*.jpg\")))\n",
        "num_val_dogs = len(list(validation_dogs_dir.glob(\"*.jpg\")))\n",
        "num_test_cats = len(list(test_cats_dir.glob(\"*.jpg\")))\n",
        "num_test_dogs = len(list(test_dogs_dir.glob(\"*.jpg\")))\n",
        "\n",
        "print(\"AFTER SPLIT:\")\n",
        "print(\"Training cats:\", num_train_cats)\n",
        "print(\"Training dogs:\", num_train_dogs)\n",
        "print(\"Validation cats:\", num_val_cats)\n",
        "print(\"Validation dogs:\", num_val_dogs)\n",
        "print(\"Test cats:\", num_test_cats)\n",
        "print(\"Test dogs:\", num_test_dogs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize a few training images\n",
        "\n",
        "Quick sanity check: we sample a few images from both classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "visualize-images"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "\n",
        "nrows, ncols = 3, 3\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(8, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "cat_images = list(train_cats_dir.glob(\"*.jpg\"))[: nrows * ncols // 2]\n",
        "dog_images = list(train_dogs_dir.glob(\"*.jpg\"))[: nrows * ncols - len(cat_images)]\n",
        "all_images = cat_images + dog_images\n",
        "\n",
        "for ax, img_path in zip(axes, all_images):\n",
        "    img = mpimg.imread(str(img_path))\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"cat\" if \"cat\" in img_path.name else \"dog\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up data generators (train / validation / test)\n",
        "\n",
        "We use `ImageDataGenerator` to rescale pixel values and stream images from disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "datagens"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "img_height = 150\n",
        "img_width = 150\n",
        "batch_size = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "val_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\",\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\",\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\",\n",
        "    shuffle=False,  # important if you later want to inspect predictions in order\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build a small convolutional neural network (Functional API)\n",
        "\n",
        "This is the same architecture as a simple `Sequential` convnet, but written in\n",
        "Keras' **Functional API** style, which makes the computation graph explicit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "build-model-functional"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(img_height, img_width, 3))\n",
        "\n",
        "x = layers.Conv2D(32, (3, 3), activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "x = layers.Conv2D(64, (3, 3), activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "x = layers.Conv2D(128, (3, 3), activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "x = layers.Conv2D(128, (3, 3), activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(512, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\n",
        "\n",
        "We use binary crossentropy loss because this is a two-class problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "train-model"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot training and validation curves\n",
        "\n",
        "This helps you diagnose overfitting and underfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plot-curves"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label=\"Training acc\")\n",
        "plt.plot(epochs_range, val_acc, label=\"Validation acc\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label=\"Training loss\")\n",
        "plt.plot(epochs_range, val_loss, label(\"Validation loss\"))\n",
        "plt.legend()\n",
        "plt.title(\"Training and validation loss\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on the held-out test set\n",
        "\n",
        "Finally, we evaluate the trained model on the `test/` directory that was never\n",
        "used during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evaluate-test"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(\"Test accuracy:\", test_acc)\n",
        "print(\"Test loss:\", test_loss)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}