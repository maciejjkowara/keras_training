{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac1e790a-b81b-47e0-b06e-5be89aca4d6b",
      "metadata": {
        "id": "ac1e790a-b81b-47e0-b06e-5be89aca4d6b"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import json\n",
        "\n",
        "# 1. Read the entire file\n",
        "raw_text = Path(\"equity_analyses.txt\").read_text(encoding=\"utf-8\")\n",
        "\n",
        "# 2. Split into individual analyses.\n",
        "#    Each analysis ends with </analysis>. There may be trailing whitespace.\n",
        "raw_blocks = re.split(r\"</analysis>\\s*\", raw_text)\n",
        "# The last split is often empty if the file ends with </analysis>\n",
        "raw_blocks = [b for b in raw_blocks if b.strip()]\n",
        "\n",
        "print(f\"Found {len(raw_blocks)} raw analyses\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5487d75a-a079-4f75-9b76-bdb003107998",
      "metadata": {
        "id": "5487d75a-a079-4f75-9b76-bdb003107998"
      },
      "outputs": [],
      "source": [
        "def extract_tag(block: str, tag: str, required: bool = False):\n",
        "    \"\"\"\n",
        "    Extracts the content inside <tag>...</tag> from the text block.\n",
        "    If required=True and the tag is not found, returns None (we can later drop those examples).\n",
        "    \"\"\"\n",
        "    # Note: for 'fund name' the tag literally is <fund name>...</fund name>\n",
        "    pattern = fr\"<{tag}>(.*?)</{tag}>\"\n",
        "    m = re.search(pattern, block, flags=re.DOTALL)\n",
        "    if not m:\n",
        "        return None\n",
        "    return m.group(1).strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "124522c8-3845-4007-adef-317c9bbeedb9",
      "metadata": {
        "id": "124522c8-3845-4007-adef-317c9bbeedb9"
      },
      "outputs": [],
      "source": [
        "records = []\n",
        "\n",
        "for i, block in enumerate(raw_blocks):\n",
        "    # Defensive: trim leading/trailing whitespace\n",
        "    b = block.strip()\n",
        "\n",
        "    # Some files may or may not have an explicit <analysis> open tag.\n",
        "    # If yours always has \"<analysis>\" at the beginning, you can strip it:\n",
        "    # b = b.replace(\"<analysis>\", \"\", 1).strip()\n",
        "\n",
        "    record = {\n",
        "        \"fund_name\":      extract_tag(b, \"fund name\"),\n",
        "        \"asset_class\":    extract_tag(b, \"asset_class\"),\n",
        "        \"category\":       extract_tag(b, \"category\"),\n",
        "        \"date\":           extract_tag(b, \"date\"),\n",
        "        \"author\":         extract_tag(b, \"author\"),\n",
        "        \"people_rating\":  extract_tag(b, \"people_rating\"),\n",
        "        \"process_rating\": extract_tag(b, \"process_rating\"),\n",
        "        \"summary\":        extract_tag(b, \"summary\"),\n",
        "        \"people\":         extract_tag(b, \"people\"),\n",
        "        \"process\":        extract_tag(b, \"process\"),\n",
        "        \"portfolio\":      extract_tag(b, \"portfolio\"),\n",
        "        \"performance\":    extract_tag(b, \"performance\"),\n",
        "    }\n",
        "\n",
        "    records.append(record)\n",
        "\n",
        "len(records)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0137fffa-0c9c-43d2-9959-48a4bb3c306a",
      "metadata": {
        "id": "0137fffa-0c9c-43d2-9959-48a4bb3c306a"
      },
      "outputs": [],
      "source": [
        "required_fields = [\"summary\", \"people\", \"process\", \"portfolio\", \"performance\"]\n",
        "\n",
        "clean_records = []\n",
        "for r in records:\n",
        "    if all(r[field] is not None and r[field].strip() for field in required_fields):\n",
        "        clean_records.append(r)\n",
        "\n",
        "print(f\"Total records: {len(records)}\")\n",
        "print(f\"Clean records with all required fields: {len(clean_records)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c233d435-5fa5-456e-82cf-35be80be6773",
      "metadata": {
        "id": "c233d435-5fa5-456e-82cf-35be80be6773"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "out_path = Path(\"equity_analyses_structured.jsonl\")\n",
        "\n",
        "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for r in clean_records:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Saved {len(clean_records)} records to {out_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f5df4d5-9c66-4764-a921-672e29bad551",
      "metadata": {
        "id": "2f5df4d5-9c66-4764-a921-672e29bad551"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab7329ea-61b2-409f-9f30-bfb64460b695",
      "metadata": {
        "id": "ab7329ea-61b2-409f-9f30-bfb64460b695"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from json import JSONDecodeError\n",
        "\n",
        "DATA_PATH = Path(\"equity_analyses_structured.jsonl\")\n",
        "\n",
        "records = []\n",
        "bad_count = 0\n",
        "\n",
        "with DATA_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f, start=1):\n",
        "        try:\n",
        "            obj = json.loads(line)\n",
        "        except JSONDecodeError as e:\n",
        "            bad_count += 1\n",
        "            print(f\"Skipping malformed line {i}: {e}\")\n",
        "            continue\n",
        "        records.append(obj)\n",
        "\n",
        "print(f\"Loaded {len(records)} valid records\")\n",
        "print(f\"Skipped {bad_count} malformed lines\")\n",
        "\n",
        "print(\"Sample keys from first record:\", records[0].keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eee4b7a1-1a61-4e5b-9cf7-3cf88eca0bb1",
      "metadata": {
        "id": "eee4b7a1-1a61-4e5b-9cf7-3cf88eca0bb1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "# Load a pretrained Flan-T5 model for zero-shot summarization\n",
        "MODEL_NAME = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(\"Model and tokenizer loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_input(rec):\n",
        "    return (\n",
        "        \"summarize_fund_analysis:\\n\"\n",
        "        \"[PEOPLE]\\n\" + rec[\"people\"] + \"\\n\\n\"\n",
        "        \"[PROCESS]\\n\" + rec[\"process\"] + \"\\n\\n\"\n",
        "        \"[PORTFOLIO]\\n\" + rec[\"portfolio\"] + \"\\n\\n\"\n",
        "        \"[PERFORMANCE]\\n\" + rec[\"performance\"]\n",
        "    )\n",
        "\n",
        "dataset = [\n",
        "    {\"input\": build_input(r), \"target\": r[\"summary\"], \"fund_name\": r[\"fund_name\"]}\n",
        "    for r in records\n",
        "]\n",
        "\n",
        "len(dataset)"
      ],
      "metadata": {
        "id": "vQQyG617iv_H"
      },
      "id": "vQQyG617iv_H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d4d7b5-c910-4536-a945-8dc1b3c9f814",
      "metadata": {
        "id": "41d4d7b5-c910-4536-a945-8dc1b3c9f814"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "def build_input_from_record(rec):\n",
        "    \"\"\"Format one analysis into a single input string for the model.\"\"\"\n",
        "    return (\n",
        "        \"summarize_fund_analysis:\\n\"\n",
        "        \"[PEOPLE]\\n\" + rec[\"people\"] + \"\\n\\n\"\n",
        "        \"[PROCESS]\\n\" + rec[\"process\"] + \"\\n\\n\"\n",
        "        \"[PORTFOLIO]\\n\" + rec[\"portfolio\"] + \"\\n\\n\"\n",
        "        \"[PERFORMANCE]\\n\" + rec[\"performance\"]\n",
        "    )\n",
        "\n",
        "# Pick one example (you can change the index to inspect others)\n",
        "example = records[0]\n",
        "input_text = build_input_from_record(example)\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(\n",
        "    input_text,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=1024,      # truncate long analyses for now\n",
        "    truncation=True,\n",
        ")\n",
        "\n",
        "# Generate a summary (zero-shot)\n",
        "output_ids = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,\n",
        "    num_beams=4,\n",
        "    length_penalty=1.0,\n",
        "    no_repeat_ngram_size=3,\n",
        ")\n",
        "\n",
        "generated_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== FUND NAME ===\")\n",
        "print(example[\"fund_name\"])\n",
        "print()\n",
        "\n",
        "print(\"=== GENERATED SUMMARY (Flan-T5, zero-shot) ===\")\n",
        "print(textwrap.fill(generated_summary, width=100))\n",
        "print()\n",
        "\n",
        "print(\"=== HUMAN SUMMARY (Morningstar analyst) ===\")\n",
        "print(textwrap.fill(example[\"summary\"], width=100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b62dc3f0-60d4-46bb-a85a-513fc895c1f4",
      "metadata": {
        "id": "b62dc3f0-60d4-46bb-a85a-513fc895c1f4"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Turn our Python list into a Hugging Face Dataset\n",
        "full_ds = Dataset.from_list(dataset)\n",
        "\n",
        "# Simple train/val split: 90% train, 10% val\n",
        "splits = full_ds.train_test_split(test_size=0.1, seed=42)\n",
        "train_ds = splits[\"train\"]\n",
        "val_ds   = splits[\"test\"]\n",
        "\n",
        "print(train_ds)\n",
        "print(val_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "599fce90-57e2-4486-ba53-a6d9372fb6d4",
      "metadata": {
        "id": "599fce90-57e2-4486-ba53-a6d9372fb6d4"
      },
      "outputs": [],
      "source": [
        "max_input_length = 1024       # truncate People/Process/Portfolio/Performance\n",
        "max_target_length = 384       # multi-paragraph summaries\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(\n",
        "        batch[\"input\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    # Tokenize targets (labels)\n",
        "    labels = tokenizer(\n",
        "        text_target=batch[\"target\"],\n",
        "        max_length=max_target_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "train_tok = train_ds.map(\n",
        "    tokenize_batch,\n",
        "    batched=True,\n",
        "    remove_columns=train_ds.column_names,\n",
        ")\n",
        "\n",
        "val_tok = val_ds.map(\n",
        "    tokenize_batch,\n",
        "    batched=True,\n",
        "    remove_columns=val_ds.column_names,\n",
        ")\n",
        "\n",
        "print(train_tok)\n",
        "print(val_tok)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "print(\"W&B disabled for this session.\")\n"
      ],
      "metadata": {
        "id": "azxEzfVYmiIs"
      },
      "id": "azxEzfVYmiIs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c75831f8-0b5b-499f-97cb-392e7677323c",
      "metadata": {
        "id": "c75831f8-0b5b-499f-97cb-392e7677323c"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./flan_t5_fund_summary\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,   # effective batch size ~8\n",
        "    num_train_epochs=1,              # keep it fast\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,      # will be used if we call trainer.evaluate()/predict later\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "import torch\n",
        "\n",
        "# Pick an index to inspect â€“ 0 is fine, or change to another integer\n",
        "idx = 0\n",
        "rec = records[idx]\n",
        "\n",
        "def build_input(rec):\n",
        "    return (\n",
        "        \"summarize_fund_analysis:\\n\"\n",
        "        \"[PEOPLE]\\n\" + rec[\"people\"] + \"\\n\\n\"\n",
        "        \"[PROCESS]\\n\" + rec[\"process\"] + \"\\n\\n\"\n",
        "        \"[PORTFOLIO]\\n\" + rec[\"portfolio\"] + \"\\n\\n\"\n",
        "        \"[PERFORMANCE]\\n\" + rec[\"performance\"]\n",
        "    )\n",
        "\n",
        "input_text = build_input(rec)\n",
        "\n",
        "# Tokenize for generation\n",
        "inputs = tokenizer(\n",
        "    input_text,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=1024,\n",
        "    truncation=True,\n",
        ")\n",
        "\n",
        "# Move to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model.to(\"cuda\")\n",
        "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "# Generate with the fine-tuned model\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        num_beams=4,\n",
        "        length_penalty=1.0,\n",
        "        no_repeat_ngram_size=3,\n",
        "    )\n",
        "\n",
        "generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== FUND NAME ===\")\n",
        "print(rec[\"fund_name\"])\n",
        "print()\n",
        "\n",
        "print(\"=== GENERATED SUMMARY (fine-tuned Flan-T5) ===\")\n",
        "print(textwrap.fill(generated, width=100))\n",
        "print()\n",
        "\n",
        "print(\"=== HUMAN SUMMARY (Morningstar analyst) ===\")\n",
        "print(textwrap.fill(rec[\"summary\"], width=100))\n"
      ],
      "metadata": {
        "id": "u9pobcqNnPt4"
      },
      "id": "u9pobcqNnPt4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CY0ChdZDrZtM"
      },
      "id": "CY0ChdZDrZtM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "file_history": [],
    "kernelspec": {
      "display_name": "morningstar-internal/analyticslab-ai:5611",
      "language": "python",
      "name": "conda-store://morningstar-internal/analyticslab-ai:5611"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}